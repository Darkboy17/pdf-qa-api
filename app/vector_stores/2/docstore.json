{"docstore/metadata": {"85c30b17-e43f-41aa-9f96-49b706971f0d": {"doc_hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de"}, "93098ea1-cab9-45ef-8277-dbecddc55dd3": {"doc_hash": "6dd06f719dab1378302b4572057c52d57e73bfaed6b6387c3e434b3c330f9b57", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "3b51f18a-4d6a-45dc-9585-04a100ba2a47": {"doc_hash": "8c081e23710f9ab74d6437de7339431603c61fa470abe91a4640b5d14f989f3d", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8": {"doc_hash": "fafbc710780410dd6cf4491647b9449c328ff506d4ece60ed90c9845a54c70a9", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "bc63e208-22f9-4518-9f45-1e354517c411": {"doc_hash": "d9d92474d920bf50c7d5b33c85e2b0ec46a10385ad17005b444d177c90dfb2c6", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "04a322e9-49c6-4d2e-843e-2163b7a67a0d": {"doc_hash": "219faca958aa6acd506e8d2eb6e694eba5de7838690983d4919ddfe36db2740a", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "ac438009-f5f7-4928-8ec0-3185e46e7c91": {"doc_hash": "821addd3eace8afd39fedcaefabdb42c854ee8b6c1e50e0bb2b156e7c1b92c9e", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74": {"doc_hash": "874f4d9a450cc1c28cbbe700b48eee8ebc806be590324182eb1e4e161d32da13", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "379af709-a618-4fc0-9312-4252dd41c4ea": {"doc_hash": "58136a4218e1a7acc391874826641f7b72877583948e8d16d253dbd1e54f3408", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}, "61ddde76-e6ce-47aa-920c-5f8b43649f48": {"doc_hash": "4a4136d12eb4b04764646c1dae03a3cc1b7b791877b638f2d110a1afc0498ae0", "ref_doc_id": "85c30b17-e43f-41aa-9f96-49b706971f0d"}}, "docstore/data": {"93098ea1-cab9-45ef-8277-dbecddc55dd3": {"__data__": {"id_": "93098ea1-cab9-45ef-8277-dbecddc55dd3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b51f18a-4d6a-45dc-9585-04a100ba2a47", "node_type": "1", "metadata": {}, "hash": "8c081e23710f9ab74d6437de7339431603c61fa470abe91a4640b5d14f989f3d", "class_name": "RelatedNodeInfo"}}, "text": "Ray Tracing in One Weekend\nPeter Shirley\n \nCopyright 2016. Peter Shirley. All rights reserved.\n \n \n \nChapter 0:  Overview\n \nI\u2019ve taught many graphics classes over the years.  Often I do them in ray tracing, because\nyou are forced to write all the code but you can still get cool images with no API. I\ndecided to adapt my course notes into a how-to, to get you to a cool program as quickly as\npossible. It will not be a full-featured ray tracer, but it does have the indirect lighting\nwhich has made ray tracing a staple in movies. Follow these steps, and the architecture of\nthe ray tracer you produce will be good for extending to a more extensive ray tracer if you\nget excited and want to pursue that.\n \nWhen somebody says \u201cray tracing\u201d it could mean many things. What I am going to\ndescribe is technically a path tracer, and a fairly general one. While the code will be pretty\nsimple (let the computer do the work!) I think you\u2019ll be very happy with the images you\ncan make.\n \nI\u2019ll take you through writing a ray tracer in the order I do it, along with some debugging\ntips. By the end, you will have a ray tracer that produces some great images. You should\nbe able to do this in a weekend.   If you take longer, don\u2019t worry about it.  I use C++ as the\ndriving language, but you don\u2019t need to. However, I suggest you do, because it\u2019s fast,\nportable, and most production movie and video game renderers are written in C++. Note\nthat I avoid most \u201cmodern features\u201d of C++, but inheritance and operator overloading are\ntoo useful for ray tracers to pass on. I do not provide the code online, but the code is real\nand I show all of it except for a few straightforward operators in the vec3 class. I am a big\nbeliever in typing in code to learn it, but when code is available I use it, so I only practice\nwhat I preach when the code is not available. So don\u2019t ask!\n \nI assume a little bit of familiarity with vectors (like dot product and vector addition). If\nyou don\u2019t know that, do a little review. If you need that review, or to learn it for the first\ntime, check out Marschner\u2019s and my graphics text, Foley, Van Dam, et al., or McGuire\u2019s\n\ngraphics codex.\n \nIf you run into trouble, or do something cool you\u2019d like to show somebody, send me some\nemail at ptrshrl@gmail.com\n \nI\u2019ll be maintaining a site related to the book including further reading and links to\nresources at a blog in1weekend related to this book. \n \nLet\u2019s get on with it!\n \n \nChapter 1:    Output an image\n \nWhenever you start a renderer, you need a way to see an image. The most straightforward\nway is to write it to a file. The catch is, there are so many formats and many of those are\ncomplex. I always start with a plain text ppm file. Here\u2019s a nice description from\nWikipedia:\n \n \n \nLet\u2019s make some C++ code to output such a thing:\n \n\n \n \nThere are some things to note in that code:\n1. The pixels are written out in rows with pixels left to right.\n2. The rows are written out from top to bottom.\n3. By convention, each of the red/green/blue components range from 0.0 to 1.0. We\nwill relax that later when we internally use high dynamic range, but before output\nwe will tone map to the zero to one range, so this code won\u2019t change.\n4. Red goes from black to fully on from left to right, and green goes from black at the\nbottom to fully on at the top. Red and green together make yellow so we should\nexpect the upper right corner to be yellow.\n \nOpening the output file (in ToyViewer on my mac, but try it in your favorite viewer and\ngoogle \u201cppm viewer\u201d if your viewer doesn\u2019t support it) shows:\n \n \n \n\nHooray!  This is the graphics \u201chello world\u201d. If your image doesn\u2019t look like that, open the\noutput file in a text editor and see what it looks like. It should start something like this:\n \n \n \nIf it doesn\u2019t, then you probably just have some newlines or something similar that is\nconfusing the image reader.\n \nIf you want to produce more image types than PPM, I am a fan of stb_image.h available\non github.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b51f18a-4d6a-45dc-9585-04a100ba2a47": {"__data__": {"id_": "3b51f18a-4d6a-45dc-9585-04a100ba2a47", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93098ea1-cab9-45ef-8277-dbecddc55dd3", "node_type": "1", "metadata": {}, "hash": "6dd06f719dab1378302b4572057c52d57e73bfaed6b6387c3e434b3c330f9b57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8", "node_type": "1", "metadata": {}, "hash": "fafbc710780410dd6cf4491647b9449c328ff506d4ece60ed90c9845a54c70a9", "class_name": "RelatedNodeInfo"}}, "text": "4. Red goes from black to fully on from left to right, and green goes from black at the\nbottom to fully on at the top. Red and green together make yellow so we should\nexpect the upper right corner to be yellow.\n \nOpening the output file (in ToyViewer on my mac, but try it in your favorite viewer and\ngoogle \u201cppm viewer\u201d if your viewer doesn\u2019t support it) shows:\n \n \n \n\nHooray!  This is the graphics \u201chello world\u201d. If your image doesn\u2019t look like that, open the\noutput file in a text editor and see what it looks like. It should start something like this:\n \n \n \nIf it doesn\u2019t, then you probably just have some newlines or something similar that is\nconfusing the image reader.\n \nIf you want to produce more image types than PPM, I am a fan of stb_image.h available\non github.\n \n \nChapter 2:   The vec3 class\n \nAlmost all graphics programs have some class(es) for storing geometric vectors and\ncolors. In many systems these vectors are 4D (3D plus a homogeneous coordinate for\ngeometry, and RGB plus an alpha transparency channel for colors). For our purposes,\nthree coordinates suffices. We\u2019ll use the same class vec3 for colors, locations, directions,\noffsets, whatever. Some people don\u2019t like this because it doesn\u2019t prevent you from doing\nsomething silly, like adding a color to a location. They have a good point, but we\u2019re going\nto always take the \u201cless code\u201d route when not obviously wrong.\n \nHere\u2019s the top part of my vec3 class:\n\n \n \nI use floats here, but in some ray tracers I have used doubles. Neither is correct\u2014 follow\nyour own tastes. Everything is in the header file, and later on in the file are lots of vector\noperations such as:\n \n \nThe / and * operations are for colors and it\u2019s unlikely you want to use them for things like\n\nlocations. Similarly, we\u2019ll need some geometric operations occasionally:\n \nAnd to make a unit vector in the same direction as the input vector:\n \n \nNow we can change our main to use this:\n \n \n \nChapter 3:  Rays, a simple camera, and background\n \nThe one thing that all ray tracers have is a ray class, and a computation of what color is\nseen along a ray. Let\u2019s think of a ray as a function p(t) = A + t*B. Here p is a 3D position\nalong a line in 3D. A  is the ray origin and B is the ray direction. The ray parameter t is a\nreal number (float in the code). Plug in a different t and p(t) moves the point along the ray.\nAdd in negative t and you can go anywhere on the 3D line. For positive t, you get only the\nparts in front of A, and this is what is often called a half-line or ray. The example C = p(2)\n\nis shown here:\n \n \n \n \n \nThe function p(t) in more verbose code form I call \u201cpoint_at_parameter(t)\u201d:\n \n \nNow we are ready to turn the corner and make a ray tracer.   At the core of a ray tracer is\nto send rays through pixels and compute what color is seen in the direction of those rays.  \nThis is of the form calculate which ray goes from the eye to a pixel, compute what that ray\nintersects, and compute a color for that intersection point.    When first developing a ray\ntracer, I always do a simple camera for getting the code up and running. I also make a\nsimple color(ray) function that returns the color of the background (a simple gradient).\n \nI\u2019ve often gotten into trouble using square images for debugging because I transpose x and\ny too often, so I\u2019ll stick with a 200x100 image. I\u2019ll put the \u201ceye\u201d (or camera center if you\nthink of a camera) at (0,0,0). I will have the y-axis go up, and the x-axis to the right. In\norder to respect the convection of a right handed coordinate system, into the screen is the\nnegative z-axis. I will traverse the screen from the lower left hand corner and use two\n\noffset vectors along the screen sides to move the ray endpoint across the screen. Note that\nI do not make the ray direction a unit length vector because I think not doing that makes\nfor simpler and slightly faster code.\n \n \n \nBelow in code, the ray r goes to approximately the pixel centers (I won\u2019t worry about\nexactness for now because we\u2019ll add antialiasing later):\n \nThe color(ray) function linearly blends white and blue depending on the up/downess of\nthe y coordinate.", "mimetype": "text/plain", "start_char_idx": 3203, "end_char_idx": 7338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8": {"__data__": {"id_": "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b51f18a-4d6a-45dc-9585-04a100ba2a47", "node_type": "1", "metadata": {}, "hash": "8c081e23710f9ab74d6437de7339431603c61fa470abe91a4640b5d14f989f3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc63e208-22f9-4518-9f45-1e354517c411", "node_type": "1", "metadata": {}, "hash": "d9d92474d920bf50c7d5b33c85e2b0ec46a10385ad17005b444d177c90dfb2c6", "class_name": "RelatedNodeInfo"}}, "text": "I\u2019ll put the \u201ceye\u201d (or camera center if you\nthink of a camera) at (0,0,0). I will have the y-axis go up, and the x-axis to the right. In\norder to respect the convection of a right handed coordinate system, into the screen is the\nnegative z-axis. I will traverse the screen from the lower left hand corner and use two\n\noffset vectors along the screen sides to move the ray endpoint across the screen. Note that\nI do not make the ray direction a unit length vector because I think not doing that makes\nfor simpler and slightly faster code.\n \n \n \nBelow in code, the ray r goes to approximately the pixel centers (I won\u2019t worry about\nexactness for now because we\u2019ll add antialiasing later):\n \nThe color(ray) function linearly blends white and blue depending on the up/downess of\nthe y coordinate. I first made it a unit vector so -1.0 < y < 1.0. I then did a standard\ngraphics trick of scaling that to 0.0 < t < 1.0. When t=1.0 I want blue. When t = 0.0 I want\n\nwhite. In between, I want a blend. This forms a \u201clinear blend\u201d, or \u201clinear interpolation\u201d, or\n\u201clerp\u201d for short, between two things. A lerp is always of the form:   blended_value = (1-\nt)*start_value + t*end_value,  with t going from zero to one. In our case this produces:\n \n \n \nChapter 4:  Adding a sphere\n \nLet\u2019s add a single object to our ray tracer. People often use spheres in ray tracers because\ncalculating whether a ray hits a sphere is pretty straightforward. Recall that the equation\nfor a sphere centered at the origin of radius R is x*x + y*y + z*z = R*R. The way you can\nread that equation is \u201cfor any (x, y, z), if x*x + y*y + z*z = R*R then (x,y,z) is on the\nsphere and otherwise it is not\u201d. It gets uglier if the sphere center is at (cx, cy, cz):\n \n(x-cx)*(x-cx) + (y-cy)*(y-cy) + (z-cz)*(z-cz)= R*R\n \nIn graphics, you almost always want your formulas to be in terms of vectors so all the\nx/y/z stuff is under the hood in the vec3 class. You might note that the vector from center C\n= (cx,cy,cz) to point p = (x,y,z) is (p - C). And dot((p - C),(p - C)) = (y-cy)*(y-cy) + (z-cz)*\n(z-cz). So the equation of the sphere in vector form is:\n \ndot((p - c),(p - c)) = R*R\n \nWe can read this as \u201cany point p that satisfies this equation is on the sphere\u201d. We want to\nknow if our ray p(t) = A + t*B ever hits the sphere anywhere. If it does hit the sphere,\nthere is some t for which p(t) satisfies the sphere equation. So we are looking for any t\nwhere this is true:\n \n\ndot((p(t) - c),(p(t) - c)) = R*R\n \nor expanding the full form of the ray p(t) :\n \ndot((A + t*B - C),(A + t*B - C)) = R*R\n \nThe rules of vector algebra are all that we would want here, and if we expand that\nequation and move all the terms to the left hand side we get:\n \nt*t*dot(B,B) + 2*t*dot(A-C,A-C) + dot(C,C) - R*R = 0\n \nThe vectors and R in that equation are all constant and known. The unknown is t, and the\nequation is a quadratic, like you probably saw in your high school math class. You can\nsolve for t and there is a square root part that is either positive (meaning two real\nsolutions), negative (meaning no real solutions), or zero (meaning one real solution). In\ngraphics, the algebra almost always relates very directly to the geometry. What we have is:\n \n \nIf we take that math and hard-code it into our program, we can test it by coloring red any\npixel that hits a small sphere we place at -1 on the z-axis:\n \n\n \nWhat we get is this:\n \n \nNow this lacks all sorts of things\u2014 like shading and reflection rays and more than one\nobject\u2014 but we are closer to halfway done than we are to our start!", "mimetype": "text/plain", "start_char_idx": 6546, "end_char_idx": 10091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc63e208-22f9-4518-9f45-1e354517c411": {"__data__": {"id_": "bc63e208-22f9-4518-9f45-1e354517c411", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8", "node_type": "1", "metadata": {}, "hash": "fafbc710780410dd6cf4491647b9449c328ff506d4ece60ed90c9845a54c70a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a322e9-49c6-4d2e-843e-2163b7a67a0d", "node_type": "1", "metadata": {}, "hash": "219faca958aa6acd506e8d2eb6e694eba5de7838690983d4919ddfe36db2740a", "class_name": "RelatedNodeInfo"}}, "text": "The unknown is t, and the\nequation is a quadratic, like you probably saw in your high school math class. You can\nsolve for t and there is a square root part that is either positive (meaning two real\nsolutions), negative (meaning no real solutions), or zero (meaning one real solution). In\ngraphics, the algebra almost always relates very directly to the geometry. What we have is:\n \n \nIf we take that math and hard-code it into our program, we can test it by coloring red any\npixel that hits a small sphere we place at -1 on the z-axis:\n \n\n \nWhat we get is this:\n \n \nNow this lacks all sorts of things\u2014 like shading and reflection rays and more than one\nobject\u2014 but we are closer to halfway done than we are to our start! One thing to be aware\nof is that we tested whether the ray hits the sphere at all, but t < 0 solutions work fine. If\nyou change your sphere center to z = +1 you will get exactly the same picture because you\nsee the things behind you. This is not a feature!  We\u2019ll fix those issues next.\n \n \nChapter 5:   Surface normals and  multiple objects.\n \nFirst, let\u2019s get ourselves a surface normal so we can shade. This is a vector that is\nperpendicular to the surface, and by convention, points out. One design decision is\nwhether these normals (again by convention) are unit length. That is convenient for\nshading so I will say yes, but I won\u2019t enforce that in the code. This could allow subtle\nbugs, so be aware this is personal preference as are most design decisions like that. For a\nsphere, the normal is in the direction of the hitpoint minus the center:\n\n \nOn the earth, this implies that the vector from the earth\u2019s center to you points straight up.\nLet\u2019s throw that into the code now, and shade it. We don\u2019t have any lights or anything yet,\nso let\u2019s just visualize the normals with a color map. A common trick used for visualizing\nnormals (because it\u2019s easy and somewhat intuitive to assume N is a unit length vector\u2013 so\neach component is between -1 and 1) is to map each component to the interval from 0 to 1,\nand then map x/y/z to r/g/b. For the normal we need the hit point, not just whether we hit\nor not. Let\u2019s assume the closest hit point (smallest t). These changes in the code let us\ncompute and visualize N:\n \n \nAnd that yields this picture:\n\n \nNow, how about several spheres? While it is tempting to have an array of spheres, a very\nclean solution is the make an \u201cabstract class\u201d for anything a ray might hit and make both a\nsphere and a list of spheres just something you can hit. What that class should be called is\nsomething of a quandary\u2014 calling it an \u201cobject\u201d would be good if not for \u201cobject\noriented\u201d programming. \u201cSurface\u201d is often used, with the weakness being maybe we will\nwant volumes. \u201cHitable\u201d emphasizes the member function that unites them. I don\u2019t love\nany of these but I will go with \u201chitable\u201d.\n \nThis hitable abstract class will have a hit function that takes in a ray. Most ray tracers have\nfound it convenient to add a valid interval for hits tmin to tmax, so the hit only \u201ccounts\u201d if\ntmin < t < tmax. For the initial rays this is positive t, but as we will see, it can help some\ndetails in the code to have an interval tmin to tmax. One design question is whether to do\nthings like compute the normal if we hit something; we might end up hitting something\ncloser as we do our search, and we will only need the normal of the closest thing. I will go\nwith the simple solution and compute a bundle of stuff I will store in some structure. I\nknow we\u2019ll want motion blur at some point, so I\u2019ll add a time input variable. Here\u2019s the\nabstract class:\n \nAnd here\u2019s the sphere (note that I eliminated a bunch of redundant 2\u2019s that cancel each\nother\n\nout):\n \nAnd a list of objects:\n\nAnd the new main:\n \n \n \n\nThis yields a picture that is really just a visualization of where the spheres are along with\ntheir surface normal. This is often a great way to look at your model for flaws and\ncharacteristics.\n \nChapter 6:  Antialiasing\n \nWhen a real camera takes a picture, there are usually no jaggies along edges because the\n\nedge pixels are a blend of some foreground and some background.", "mimetype": "text/plain", "start_char_idx": 9370, "end_char_idx": 13500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04a322e9-49c6-4d2e-843e-2163b7a67a0d": {"__data__": {"id_": "04a322e9-49c6-4d2e-843e-2163b7a67a0d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc63e208-22f9-4518-9f45-1e354517c411", "node_type": "1", "metadata": {}, "hash": "d9d92474d920bf50c7d5b33c85e2b0ec46a10385ad17005b444d177c90dfb2c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac438009-f5f7-4928-8ec0-3185e46e7c91", "node_type": "1", "metadata": {}, "hash": "821addd3eace8afd39fedcaefabdb42c854ee8b6c1e50e0bb2b156e7c1b92c9e", "class_name": "RelatedNodeInfo"}}, "text": "I will go\nwith the simple solution and compute a bundle of stuff I will store in some structure. I\nknow we\u2019ll want motion blur at some point, so I\u2019ll add a time input variable. Here\u2019s the\nabstract class:\n \nAnd here\u2019s the sphere (note that I eliminated a bunch of redundant 2\u2019s that cancel each\nother\n\nout):\n \nAnd a list of objects:\n\nAnd the new main:\n \n \n \n\nThis yields a picture that is really just a visualization of where the spheres are along with\ntheir surface normal. This is often a great way to look at your model for flaws and\ncharacteristics.\n \nChapter 6:  Antialiasing\n \nWhen a real camera takes a picture, there are usually no jaggies along edges because the\n\nedge pixels are a blend of some foreground and some background. We can get the same\neffect by averaging a bunch of samples inside each pixel. We will not bother with\nstratification, which is controversial but is usual for my programs.  For some ray tracers it\nis critical, but the kind of general one we are writing doesn\u2019t benefit very much from it and\nit makes the code uglier.  We abstract the camera class a bit so we can make a cooler\ncamera later.\n \nOne thing we need is a random number generator that returns real random numbers. C++\ndid not traditionally have a standard random number generator but most systems have\ndrand48() tucked away someplace and that is what I use here. However, newer versions of\nC++ have addressed this issue with the <random> header (if imperfectly according to\nsome experts). Whatever your infrastructure, find a function that returns a canonical\nrandom number which by convention returns  random real in the range 0 <= ran < 1. The\n\u201cless than\u201d before the 1 is important as we will sometimes take advantage of that.\n \nFor a given pixel we have several samples within that pixel and send rays through each of\nthe samples. The colors of these rays are then averaged:\n \n \n \nPutting that all together yields a camera class encapsulating our simple axis-aligned\ncamera from before:\n\n \nMain is also changed:\n \n \nZooming into the image that is produced, the big change is in edge pixels that are part\nbackground and part foreground:\n\n \n \nChapter 7:    Diffuse Materials\n \nNow that we have objects and multiple rays per pixel, we can make some realistic looking\nmaterials. We\u2019ll start with diffuse (matte) materials. One question is whether we can mix\nand match shapes and materials (so we assign a sphere a material) or if it\u2019s put together so\nthe geometry and material are tightly bound (that could be useful for procedural objects\nwhere the geometry and material are linked). We\u2019ll go with separate\u2014 which is usual in\nmost renderers\u2014 but do be aware of the limitation.\n \nDiffuse objects that don\u2019t emit light merely take on the color of their surroundings, but\nthey modulate that with their own intrinsic color. Light that reflects off a diffuse surface\nhas its direction randomized. So, if we send three rays into a crack between two diffuse\nsurfaces they will each have different random behavior:\n \n \n\n \nThey also might be absorbed rather than reflected. The darker the surface, the more likely\nabsorption is. (That\u2019s why it is dark!) Really any algorithm that randomizes direction will\nproduce surfaces that look matte. One of the simplest ways to do this turns out to be\nexactly correct for ideal diffuse surfaces. (I used to do it as a lazy hack, but a commenter\non my blog showed that it was in fact mathematically ideal Lambertian.)\n \nPick a random point s from the unit radius sphere that is tangent to the hitpoint, and send a\nray from the hitpoint p to the random point s. That sphere has center (p+N):\n \n \n \nWe also need a way to pick a random point in a unit radius sphere centered at the origin.\nWe\u2019ll use what is usually the easiest algorithm: a rejection method. First, we pick a\nrandom point in the unit cube where x, y, and z all range from -1 to +1. We reject this point\nand try again if the point is outside the sphere. A do/while construct is perfect for that:\nThis gives us:\n\nNote the shadowing under the sphere.   This picture is very dark, but our spheres only\nabsorb half the energy on each bounce, so they are 50% reflectors.   If you can\u2019t see the\nshadow, don\u2019t worry, we will fix that now.  These spheres should look pretty light (in real\nlife, a light grey).", "mimetype": "text/plain", "start_char_idx": 12765, "end_char_idx": 17054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac438009-f5f7-4928-8ec0-3185e46e7c91": {"__data__": {"id_": "ac438009-f5f7-4928-8ec0-3185e46e7c91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a322e9-49c6-4d2e-843e-2163b7a67a0d", "node_type": "1", "metadata": {}, "hash": "219faca958aa6acd506e8d2eb6e694eba5de7838690983d4919ddfe36db2740a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74", "node_type": "1", "metadata": {}, "hash": "874f4d9a450cc1c28cbbe700b48eee8ebc806be590324182eb1e4e161d32da13", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019ll use what is usually the easiest algorithm: a rejection method. First, we pick a\nrandom point in the unit cube where x, y, and z all range from -1 to +1. We reject this point\nand try again if the point is outside the sphere. A do/while construct is perfect for that:\nThis gives us:\n\nNote the shadowing under the sphere.   This picture is very dark, but our spheres only\nabsorb half the energy on each bounce, so they are 50% reflectors.   If you can\u2019t see the\nshadow, don\u2019t worry, we will fix that now.  These spheres should look pretty light (in real\nlife, a light grey). The reason for this is that almost all image viewers assume that the\nimage is \u201cgamma corrected\u201d, meaning the 0 to 1 values have some transform before being\nstored as a byte. There are many good reasons for that, but for our purposes we just need\nto be aware of it. To a first approximation, we can use \u201cgamma 2\u201d which means raising the\ncolor to the power 1/gamma, or in our simple case \u00bd, which is just square-root:\n \nThat yields light grey, as we desire:\n \n \nChapter 8:  Metal\n \nIf we want different objects to have different materials, we have a design decision. We\ncould have a universal material with lots of parameters and different material types just\nzero out some of those parameters. This is not a bad approach. Or we could have an\n\nabstract material class that encapsulates behavior. I am a fan of the latter approach. For our\nprogram the material needs to do two things:\n \n1. produce a scattered ray (or say it absorbed the incident ray)\n2. if scattered, say how much the ray should be attenuated\n \nThis suggests the abstract class:\n \nThe hit_record is to avoid a bunch of arguments so we can stuff whatever info we want in\nthere.   You can use arguments instead; it\u2019s a matter of taste.    Hitables and materials need\nto know each other so there is some circularity of the references.   In C++ you just need to\nalert the compiler that the pointer is to a class, which the \u201cclass material\u201d in the hitable\nclass below does:\n \n \nFor the Lambertian (diffuse) case we already have, it can either scatter always and\nattenuate by its reflectance R, or it can scatter with no attenuation but absorb the fraction\n1-R of the rays. Or it could be a mixture of those strategies. For Lambertian materials we\nget this simple class: \n \n\nNote we could just as well only scatter with some probability p and have attenuation be\nalbedo/p. Your choice.\n \nFor smooth metals the ray won\u2019t be randomly scattered. The key math is: how does a ray\nget reflected from a metal mirror?   Vector math is our friend here:\n \n \n \nThe reflected ray direction in red is just (v + 2B). In our design, N is a unit vector, but v\nmay not be. The length of B should be dot(v,N). Because v points in, we will need a minus\nsign yielding:\n \nThe metal material just reflects rays using that formula:\n \n\nWe need to modify the color function to use this:\nAnd add some metal spheres:\n \nWhich gives:\n\n \nWe can also randomize the reflected direction by using a small sphere and choosing a new\nendpoint for the ray:\n \n \nThe bigger the sphere, the fuzzier the reflections will be. This suggests adding a fuzziness\nparameter that is just the radius of the sphere (so zero is no perturbation). The catch is that\nfor big spheres or grazing rays, we may scatter below the surface. We can just have the\nsurface absorb those. We\u2019ll put a maximum of 1 on the radius of the sphere which yields:\n \n \n \n \nWe can try that out by adding fuzziness 0.3 and 1.0 to the metals:\n \n\n \nChapter 9:  Dielectrics\n \nClear materials such as water, glass, and diamonds are dielectrics. When a light ray hits\nthem, it splits into a reflected ray and a refracted (transmitted) ray. We\u2019ll handle that by\nrandomly choosing between reflection or refraction and only generating one scattered ray\nper interaction.\n \nThe hardest part to debug is the refracted ray. I usually first just have all the light refract if\nthere is a refraction ray at all. For this project, I tried to put two glass balls in our scene,\nand I got this:\nIs that right?   Glass balls look odd in real life. But no, it isn\u2019t right.", "mimetype": "text/plain", "start_char_idx": 16477, "end_char_idx": 20587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74": {"__data__": {"id_": "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac438009-f5f7-4928-8ec0-3185e46e7c91", "node_type": "1", "metadata": {}, "hash": "821addd3eace8afd39fedcaefabdb42c854ee8b6c1e50e0bb2b156e7c1b92c9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "379af709-a618-4fc0-9312-4252dd41c4ea", "node_type": "1", "metadata": {}, "hash": "58136a4218e1a7acc391874826641f7b72877583948e8d16d253dbd1e54f3408", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019ll put a maximum of 1 on the radius of the sphere which yields:\n \n \n \n \nWe can try that out by adding fuzziness 0.3 and 1.0 to the metals:\n \n\n \nChapter 9:  Dielectrics\n \nClear materials such as water, glass, and diamonds are dielectrics. When a light ray hits\nthem, it splits into a reflected ray and a refracted (transmitted) ray. We\u2019ll handle that by\nrandomly choosing between reflection or refraction and only generating one scattered ray\nper interaction.\n \nThe hardest part to debug is the refracted ray. I usually first just have all the light refract if\nthere is a refraction ray at all. For this project, I tried to put two glass balls in our scene,\nand I got this:\nIs that right?   Glass balls look odd in real life. But no, it isn\u2019t right. The world should be\nflipped upside down and no weird black stuff. I just printed out the ray straight through the\nmiddle of the image and it was clearly wrong. That often does the job.\n \nThe refraction is described by Snell\u2019s law:\n \nn sin(theta) = n\u2019 sin(theta\u2019)\n \nWhere n and n\u2019 are the refractive indices (typically air = 1, glass = 1.3-1.7, diamond = 2.4)\n\nand the geometry is:\n \n \nOne troublesome practical issue is that when the ray is in the material with the higher\nrefractive index, there is no real solution to Snell\u2019s law and thus there is no refraction\npossible. Here all the light is reflected, and because in practice that is usually inside solid\nobjects, it is called \u201ctotal internal reflection\u201d. This is why sometimes the water-air\nboundary acts as a perfect mirror when you are submerged. The code for refraction is thus\na bit more complicated than for reflection:\n \n \n \nAnd the dielectric material that always refracts when possible is:\n\nAttenuation is always 1\u2014 the glass surface absorbs nothing.\n \nIf we try that out with these parameters:\nWe get:\n \n(The reader Becker has pointed out that when there is a reflection ray the function returns\nfalse so there are no reflections.   He is right and that is why there are none in the image\nabove.   I am leaving this in rather than correcting this because it is a very interesting\nexample of a major bug that still leaves a reasonably plausible image.   These sleeper bugs\nare the hardest bugs to find because we humans are not designed to find fault with what\nwe see.)\n \nNow real glass has reflectivity that varies with angle\u2014 look at a window at a steep angle\nand it becomes a mirror. There is a big ugly equation for that, but almost everybody uses a\nsimple and surprisingly simple polynomial approximation by Christophe Schlick:\n\nThis yields our full glass material:\n \n \nAn interesting and easy trick with dielectric spheres is to note that if you use a negative\nradius, the geometry is unaffected but the surface normal points inward, so it can be used\nas a bubble to make a hollow glass sphere:\nThis gives:\n \n\nChapter 10:  Positionable camera\n \nCameras, like dielectrics, are a pain to debug. So I always develop mine incrementally.\nFirst, let\u2019s allow an adjustable field of view (fov). This is the angle you see through the\nportal. Since our image is not square, the fov is different horizontally and vertically. I\nalways use vertical fov. I also usually specify it in degrees and change to radians inside a\nconstructor\u2014 a matter of personal taste.\n \nI first keep the rays coming from the origin and heading to the z=-1 plane. We could make\nit the z=-2 plane, or whatever, as long as we made h a ratio to that distance. Here is our\nsetup:\n \n \nThis implies h = tan(theta/2). Our camera now becomes:\n \nWith calling it camera cam(90, float(nx)/float(ny)) and these spheres:\n\ngives:\n \nTo get an arbitrary viewpoint, let\u2019s first name the points we care about. We\u2019ll call the\nposition where we place the camera lookfrom, and the point we look at lookat. (Later, if\nyou want, you could define a direction to look in instead of a point to look at.)\n \nWe also need a way to specify the roll, or sideways tilt, of the camera; the rotation around\nthe lookat-lookfrom axis. Another way to think about it is even if you keep lookfrom and\nlookat constant, you can still rotate your head around your nose. What we need is a way to\nspecify an up vector for the camera.", "mimetype": "text/plain", "start_char_idx": 19836, "end_char_idx": 24011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "379af709-a618-4fc0-9312-4252dd41c4ea": {"__data__": {"id_": "379af709-a618-4fc0-9312-4252dd41c4ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74", "node_type": "1", "metadata": {}, "hash": "874f4d9a450cc1c28cbbe700b48eee8ebc806be590324182eb1e4e161d32da13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61ddde76-e6ce-47aa-920c-5f8b43649f48", "node_type": "1", "metadata": {}, "hash": "4a4136d12eb4b04764646c1dae03a3cc1b7b791877b638f2d110a1afc0498ae0", "class_name": "RelatedNodeInfo"}}, "text": "Here is our\nsetup:\n \n \nThis implies h = tan(theta/2). Our camera now becomes:\n \nWith calling it camera cam(90, float(nx)/float(ny)) and these spheres:\n\ngives:\n \nTo get an arbitrary viewpoint, let\u2019s first name the points we care about. We\u2019ll call the\nposition where we place the camera lookfrom, and the point we look at lookat. (Later, if\nyou want, you could define a direction to look in instead of a point to look at.)\n \nWe also need a way to specify the roll, or sideways tilt, of the camera; the rotation around\nthe lookat-lookfrom axis. Another way to think about it is even if you keep lookfrom and\nlookat constant, you can still rotate your head around your nose. What we need is a way to\nspecify an up vector for the camera. Notice we already we already have a plane that the up\nvector should be in, the plane orthogonal to the view direction.\n \n \nWe can actually use any up vector we want, and simply project it onto this plane to get an\nup vector for the camera. I use the common convention of naming a \u201cview up\u201d (vup)\nvector. A couple of cross products, and we now have a complete orthonormal basis (u,v,w)\nto describe our camera\u2019s orientation.\n\n \nRemember that vup, v, and w are all in the same plane. Note that, like before when our\nfixed camera faced -Z, our arbitrary view camera faces -w. And keep in mind that we can\n\u2014 but we don\u2019t have to\u2014 use world up (0,1,0) to specify vup. This is convenient and will\nnaturally keep your camera horizontally level until you decide to experiment with crazy\ncamera angles.\n \n \nThis allows us to change the viewpoint:\n \nto get:\n\n \nAnd we can change field of view to get:\n \nChapter 11:   Defocus Blur\n \nNow our final feature: defocus blur. Note, all photographers will call it \u201cdepth of field\u201d so\nbe aware of only using \u201cdefocus blur\u201d among friends.\n \nThe reason we defocus blur in real cameras is because they need a big hole (rather than\njust a pinhole) to gather light.   This would defocus everything, but if we stick a lens in the\nhole, there will be a certain distance where everything is in focus.   The distance to that\nplane where things are in focus is controlled by the distance between the lens and the\nfilm/sensor.   That is why you see the lens move relative to the camera when you change\nwhat is in focus (that may happen in your phone camera too, but the sensor moves).   The\n\u201caperture\u201d is a hole to control how big the lens is effectively.   For a real camera, if you\nneed more light you make the aperture bigger, and will get more defocus blur.   For our\nvirtual camera, we can have a perfect sensor and never need more light, so we only have\nan aperture when we want defocus blur.\n \nA real camera has a compound lens that is complicated. For our code we could simulate\nthe order: sensor,  then lens,  then aperture, and figure out where to send the rays and flip\nthe image once computed (the image is projected upside down on the film). Graphics\npeople usually use a thin lens approximation.  \n\n \n \nWe also don\u2019t need to simulate any of the inside of the camera. For the purposes of\nrendering an image outside the camera, that would be unnecessary complexity. Instead I\nusually start rays from the surface of the lens, and send them toward a virtual film plane,\nby finding the projection of the film on the plane that is in focus (at the distance\nfocus_dist).     \n \n \n \n \nFor that we just need to have the ray origins be on a disk around lookfrom rather than\nfrom a point:\n \n \n \n \n \n \n \n\n \nUsing a big aperture:\n \nWe get:\n \nChapter 12:  Where next?\n \nFirst let\u2019s make the image on the cover of this book\u2014 lots of random spheres:\n\nThis gives:\n \n \nAn interesting thing you might note is the glass balls don\u2019t really have shadows which\nmakes them look like they are floating.   This is not a bug (you don\u2019t see glass balls much\nin real life, where they also look a bit strange and indeed seem to float on cloudy days).  \nA point on the big sphere under a glass ball still has lots of light hitting it because the sky\nis re-ordered rather than blocked.\n \n \nYou now have a cool ray tracer!   What next?\n\n \n1. Lights. You can do this explicitly, by sending shadow rays to lights.", "mimetype": "text/plain", "start_char_idx": 23279, "end_char_idx": 27423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61ddde76-e6ce-47aa-920c-5f8b43649f48": {"__data__": {"id_": "61ddde76-e6ce-47aa-920c-5f8b43649f48", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c30b17-e43f-41aa-9f96-49b706971f0d", "node_type": "4", "metadata": {}, "hash": "6b8a829270a793da42ce1c3f75c9203c8db4621a0799bb72555a069cd89835de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379af709-a618-4fc0-9312-4252dd41c4ea", "node_type": "1", "metadata": {}, "hash": "58136a4218e1a7acc391874826641f7b72877583948e8d16d253dbd1e54f3408", "class_name": "RelatedNodeInfo"}}, "text": "For that we just need to have the ray origins be on a disk around lookfrom rather than\nfrom a point:\n \n \n \n \n \n \n \n\n \nUsing a big aperture:\n \nWe get:\n \nChapter 12:  Where next?\n \nFirst let\u2019s make the image on the cover of this book\u2014 lots of random spheres:\n\nThis gives:\n \n \nAn interesting thing you might note is the glass balls don\u2019t really have shadows which\nmakes them look like they are floating.   This is not a bug (you don\u2019t see glass balls much\nin real life, where they also look a bit strange and indeed seem to float on cloudy days).  \nA point on the big sphere under a glass ball still has lots of light hitting it because the sky\nis re-ordered rather than blocked.\n \n \nYou now have a cool ray tracer!   What next?\n\n \n1. Lights. You can do this explicitly, by sending shadow rays to lights. Or it can be\ndone implicitly by making some objects emit light,\n2. biasing scattered rays toward them, and then downweighting those rays to cancel\nout the bias. Both work. I am in the minority in favoring the latter approach.\n3. Triangles. Most cool models are in triangle form. The model I/O is the worst and\nalmost everybody tries to get somebody else\u2019s code to do this.\n4. Surface textures. This lets you paste images on like wall paper. Pretty easy and a\ngood thing to do.\n5. Solid textures. Ken Perlin has his code online. Andrew Kensler has some very cool\ninfo at his blog.\n6. Volumes and media. Cool stuff and will challenge your software architecture. I\nfavor making volumes have the hitable interface and probabilistically have\nintersections based on density. Your rendering code doesn\u2019t even have to know it\nhas volumes with that method.\n7. Parallelism. Run N copies of your code on N cores with different random seeds.\nAverage the N runs. This averaging can also be done hierarchically where N/2 pairs\ncan be averaged to get N/4 images, and pairs of those can be averaged. That\nmethod of parallelism should extend well into the thousands of cores with very\nlittle coding.\n \nHave fun, and please send me your cool images!\n \n \n \nAcknowledgements\n \nThanks to readers Becker and Lorenzeo Mancini for finding bugs, and to the limnu.com\nteam with help on the figures.", "mimetype": "text/plain", "start_char_idx": 26622, "end_char_idx": 28796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"85c30b17-e43f-41aa-9f96-49b706971f0d": {"node_ids": ["93098ea1-cab9-45ef-8277-dbecddc55dd3", "3b51f18a-4d6a-45dc-9585-04a100ba2a47", "c0384c2c-abaf-4cc8-9190-f1316f8eb1e8", "bc63e208-22f9-4518-9f45-1e354517c411", "04a322e9-49c6-4d2e-843e-2163b7a67a0d", "ac438009-f5f7-4928-8ec0-3185e46e7c91", "a32b0a28-62bf-4d1e-9d5f-b4e19ba06c74", "379af709-a618-4fc0-9312-4252dd41c4ea", "61ddde76-e6ce-47aa-920c-5f8b43649f48"], "metadata": {}}}}